## 分类问题

### logistics和二分类



$$
sigmoid(x) = \frac{1}{1+e^{-x}}
$$

## 独热编码

使用独热编码去表示不同的类别

对于数据集中n个不同类别，我们创造长度为n的向量来表示不同的类别，这个向量第i个位置为1，其余位置为0，就表示第i类。

### softmax和多分类

softmax是一个归一化函数
希望将结果映射到[0,1]之间，对于分类问题而言，我们有N个类别，我们希望他们在各个类别上得到值的和为1，这样就可以表示该样本点属于各个类别的概率。
$$
softmax(x_i)=\frac{e^{x_i}}{\sum_{j=1}^Ne^{x_j}}
$$

```python
import torch
import torch.nn.functional as f

y = torch.tensor([0.7, 0.9, 0.4], dtype=torch.float)
y = f.softmax(y)

print(y)
```



### 交叉熵 Cross Entropy 

分类问题的损失函数


$$
H(p, q) = - \sum_{i=1}^{n} p(x_i) \log q(x_i) \\
= - \sum_{i=1}^{n} y_i \log \hat{y}_i
$$



n = 3

y = 1

y_hat = [0.7, 0.9, 0.4]

现在是一个多分类问题，样本一共有3个不同类别，当前样本是第二类，标签为y（索引从0开始），模型输出为y_hat

计算交叉熵：

- 对标签进行独热编码，得到[0, 1, 0]
- 对模型输出做softmax映射，得到[0.3376, 0.4123, 0.2501]
- 使用交叉熵计算[0, 1, 0]和[0.3376, 0.4123, 0.2501]两个概率分布之间的差异








```python
import torch
import torch.nn.functional as f

# 使用torch自带的方法计算交叉熵
y_real = torch.tensor([1], dtype=torch.long)
y_hat = torch.tensor([[0.7, 0.9, 0.4]], dtype=torch.float)

loss = f.cross_entropy(y_hat, y_real)
print(loss)

# 自己计算交叉熵
y_real = torch.tensor([0, 1, 0], dtype=torch.float)
y_hat = torch.tensor([0.7, 0.9, 0.4], dtype=torch.float)

y_hat = f.softmax(y_hat)

loss = - torch.sum(y_real * torch.log(y_hat))
print(loss)
```







## MNIST数据集

MNIST数据集是一个的手写数字图像数据集，通常用于训练各种图像处理系统。该数据集也被广泛用于机器学习领域的训练和测试。



图像与矩阵

矩阵的每个元素对应图像的一个像素点，元素的值代表了色彩







```python
import numpy as np
import matplotlib.pyplot as plt

number = np.array([[1, 0, 1],
                   [1, 0, 1],
                   [1, 1, 1],
                   [0, 0, 1],
                   [0, 0, 1]])

plt.imshow(number, cmap='binary')
plt.show()
```

查看自己电脑上的图片及其矩阵

```python
import matplotlib.pyplot as plt

p = 'D:/a.jpg'
im = plt.imread(p)
print(im)
plt.imshow(im)
plt.show()
```

MNIST数据集读取与展示

```python
import csv
import matplotlib.pyplot as plt
import numpy as np

p = r'C:\Users\Guppy\Documents\GitHub\DeepLearningNotebook\datasets\mnist\mnist.csv'

with open(p, 'r') as f:  # 读取csv文件
    data = list(csv.reader(f))  # 读文件并保存为列表
    data = data[1::]  # 全部数据，取第二行到最后一行，跳过第一行的标题
    data = np.array(data, dtype=np.float)  # 把数据转为array

x = data[:, 1::] / 255  # 图像数据，取第二列到最后一列
y = data[:, 0]  # 标签数据，第一列

index = 20
mat = x[index].reshape(28, 28)  # 改变形状，还原为28x28大小的矩阵
plt.imshow(mat, cmap='binary')  # 以矩阵形式显示图像
plt.title(y[index])  # 以该图像矩阵对应的标签作为图像的标题
plt.show()
```



```python
"""
定义模型
确定损失函数以及优化方法
实现模型训练的过程(前向传播，计算损失，反向传播，更新参数)

处理数据
封装数据
实例化模型
执行训练的过程
测试和验证模型
"""
import torch
import torch.nn as nn
import torch.nn.functional as fn
import torch.optim as optim
import torch.utils.data
import csv
import numpy as np
import random


# 定义模型
class NN(nn.Module):
    def __init__(self):
        super(NN, self).__init__()

        self.l1 = nn.Linear(784, 512)
        self.l2 = nn.Linear(512, 256)
        self.l3 = nn.Linear(256, 10)
        self.activate = nn.Sigmoid()

    def forward(self, x):
        out = self.activate(self.l1(x))
        out = self.activate(self.l2(out))
        out = self.l3(out)

        return out


def training(model, loader, opt, epochs):
    for epoch in range(epochs):
        total_loss = 0
        for x, y in loader:
            # 梯度清零
            opt.zero_grad()

            # 前向传播
            y_ = model(x)
            y_ = y_.squeeze()  # 去掉多余的维度

            # 计算损失
            loss = fn.cross_entropy(y_, y, reduction='sum')
            total_loss += loss

            # 反向传播
            loss.backward()

            # 更新模型参数
            opt.step()

        total_loss = total_loss / len(loader.dataset)
        print(total_loss.item())


if __name__ == '__main__':
    p = r'C:\Users\Guppy\Documents\GitHub\DeepLearningNotebook\datasets\mnist\mnist.csv'

    with open(p, 'r') as f:  # 读取csv文件
        data = list(csv.reader(f))  # 读文件并保存为列表
        data = data[1::]  # 全部数据，取第二行到最后一行，跳过第一行的标题
        data = np.array(data, dtype=np.float)  # 把数据转为array

    # 划分数据集
    rate_of_train = 0.8  # 训练样本占比
    n_of_train = int(rate_of_train * len(data))  # 训练样本数目
    data_train = data[0:n_of_train]  # 训练集
    data_test = data[n_of_train::]  # 测试集

    x_train = data_train[:, 1::] / 255
    y_train = data_train[:, 0]

    x_train = torch.from_numpy(x_train).float()  # array转为tensor
    y_train = torch.from_numpy(y_train).long()
    ds = torch.utils.data.TensorDataset(x_train, y_train)  # 封装为TensorDataset
    loader = torch.utils.data.DataLoader(ds, batch_size=100)  # 封装为DataLoader

    # 模型初始化
    model = NN()
    # 优化器初始化
    opt = optim.Adam(params=model.parameters(), lr=0.01)
    # 执行训练过程
    training(model=model, loader=loader, opt=opt, epochs=5)

    # 测试模型
    x_test = data_test[:, 1::] / 255
    y_test = data_test[:, 0]

    x_test = torch.from_numpy(x_test).float()  # array转tensor
    y_test = torch.from_numpy(y_test).long()  # array转tensor

    y_predict = model(x_test)  # 模型输出
    y_predict = y_predict.squeeze()

    prediction = torch.argmax(y_predict, 1)  # 模型预测的标签
    correct = (prediction == y_test).sum().float()  # 统计模型预测标签等于目标标签的数目
    accuracy = correct / y_test.size(0)  # 计算准确率
    print('测试准确率为: {}'.format(accuracy.item()))

    loss = fn.cross_entropy(y_predict, y_test)
    print('测试损失为: {}'.format(loss.item()))

```



```
prediction = torch.argmax(y_predict, 1)
correct = (prediction == y_test).sum().float()
accuracy = correct / y_test.size(0)
print(accuracy.item())
```





