# Pytorch 基础

官网 https://pytorch.org/

官方教程 https://pytorch.org/tutorials/

官方文档 https://pytorch.org/docs/stable/index.html



## 1. Tensor 张量

Tensors 类似于 NumPy 的 ndarrays ，同时 Tensors 可以使用 GPU 进行计算。支持自动求导。

## 2. Tensor 的创建

指定形状，构造一个随机的张量


```python
x = torch.rand(2, 3)
print(x)
```


构造一个全为0的张量，并指定其数据类型是

```python
x = torch.zeros(2, 3, dtype=torch.long)
print(x)
```


根据已有数据构造张量

```python
x = torch.tensor([1, 2.])
print(x)
```


基于已有张量创建一个同形状的张量。

```python
x = x.new_ones(3, 2, dtype=torch.double)
print(x)
y = torch.randn_like(x, dtype=torch.float)
print(y)
```



## 3. Tensor的操作



索引和切片

```python
x = torch.randn(4, 4)
print(x)
print(x[:, 0])
print(x[0])
print(x[1, 1])
print(x[2:4, 2:4])
```




变形

```python
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8) # the size -1 is inferred from other dimensions
print(x.size(), y.size(), z.size())
```



取值

```python
x = torch.randn(1)
print(x)
print(x.item())
```



形状

```python
x = torch.randn(2, 3)
print(x.size())
```

tensor.size() 是一个元组




加法

```python
x = torch.rand(2, 2)
y = torch.rand(2, 2)
print(x)
print(y)
print(x + y)
print(torch.add(x, y))
print(x.add(y))
print(x.add_(y))
```



```python

```



```python
print(x)
x.add(y)
print(x)
x.add_(y)
print(x)
```



注意 任何使张量会发生变化的操作都有一个前缀 '_'

x.copy_(y)

x.t_()



## 4. 自动求导

链式法则


$$
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial t} \frac{\partial t}{\partial x}
$$


数据流图，计算图

节点（node）代表对数据所做的某种运算或操作

边（edge）代表从一个节点传入另一个节点的实际数值



例如：

有x个苹果，单价w，b是杂费，y是总价。y的函数用公式表示为：


$$
y = w*x+b
$$

用计算图表示为，用圆形表示节点，用连线表示边：


```mermaid
graph LR

11[个数 x] -->|2| 21((x))
12[单价 w] -->|3| 21


21 -->|6| 31((+))
22[偏置 b] -->|4| 31


31 -->|10| 51[总价 y]
```

图中从左到右是一种正向传播forward，即从输入到输出的传播过程。

从输出到输入是一种反向传播backward，反向传播导数。对一个操作节点，该节点沿着其某个输入方向的反向传播就是该节点的输出关于这个输入的偏导数。

```mermaid
graph LR

1[x] -->|x| 2((f))

2 -->|y| 3[y]

2 -->|f'x| 1

```

计算图的优点：

1. 局部计算，同层级节点间的计算是独立的，各节点只需要考虑自己相关的计算，不用考虑全局。

2. 便于使用反向传播求导。





pytorch中tensor的一些属性

```
tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor
 
参数:
    data： (array_like): tensor的初始值. 可以是列表，元组，numpy数组，标量等;
    dtype： tensor元素的数据类型
    device： 指定CPU或者是GPU设备，默认是None
    requires_grad：是否可以求导，即求梯度，默认是False，即不可导的
```



https://pytorch.org/docs/stable/generated/torch.tensor.html?highlight=tensor#torch.tensor



如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。完成计算后，可以调用 .backward() 来自动计算所有梯度。该张量的梯度将累积到 .grad 属性中。

调用 .detach()可以停止 tensor 历史记录的跟踪，它将其与计算历史记录分离，并防止将来的计算被跟踪。

要停止跟踪历史记录（和使用内存），还可以将代码块使用 with torch.no_grad(): 包装起来。在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad = True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。

Tensor 和 Function 互相连接并构建成计算图，tensor为边，function为节点。它保存整个完整的计算过程的历史信息。每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，也就是计算图中输出这个张量的操作节点的引用（如果用户自己创建张量，则g rad_fn 是 None ）。



```python
import torch

x = torch.tensor(2., requires_grad=True)  # pytorch求导要求是浮点数不能是整数
w = torch.tensor(3., requires_grad=True)
b = torch.tensor(4., requires_grad=True)

h = w * x
y = h + b

print(y.requires_grad)

y.backward()  # 反向传播计算导数

print(x.grad)  # y关于x的导数

print(h.grad_fn)  # 计算出h的function
print(y.grad_fn)  # 计算出y的function
```



