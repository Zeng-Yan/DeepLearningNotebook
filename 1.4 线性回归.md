# 线性回归

### 模型定义

给定m个样本，每个样本包含n个特征的输入x，标签y是连续值

线性回归模型定义为：
$$
\hat{y} = \sum_{i=1}^{n} w_ix_i + b \\
=\vec{w} \cdot \vec{x} + b \tag{1}
$$

### 误差评价

误差函数，损失函数

误差，去度量y和y_hat之间的误差

MSE（Mean Square Error）均方差：
$$
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 \tag{2} 
$$

### 使用最小二乘法求解参数

由于输入和输出是给定的，MSE实际上是关于参数w和b的函数，将(1)带入(2)可得
$$
MSE(w, b) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2  \tag{3}
$$
我们的目的是使均方差最小，就是求w和b使得MSE最小。

更进一步，MSE是关于w和b的凸函数，当其关于w和b的偏导数均为0时，MSE取得最小值。即求w和b使得
$$
\frac{\partial MSE}{ \partial w} = 0 \tag{4}
$$

$$
\frac{\partial MSE}{ \partial b} = 0 \tag{5}
$$

可以解得：
$$
\begin{equation}
\left\{
\begin{aligned}

\vec{w} = \frac{\sum y_i (\vec{x}_i - \overline{x}) }{\sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2} \\

b = \frac{1}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i)

\end{aligned}
\right.
\end{equation}
$$

推导过程如下：

计算MSE关于w的偏导
$$
\tag{6}

\frac{\partial MSE}{ \partial w} 
= \frac{ \partial[\frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]  }{ \partial w} \\

= \frac{1}{m} \frac{\partial [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]}{\partial w} \\

= \frac{1}{m}  \sum_{i=1}^{m} 2(y_i - \vec{w} \cdot \vec{x}_i - b)(-\vec{x}_i) \\ 

= -\frac{2}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b) \vec{x}_i  \\ 

= -\frac{2}{m} \sum_{i=1}^{m} (y_i \vec{x}_i - b \vec{x}_i - \vec{w} \cdot \vec{x}_i^2 )  \\

= -\frac{2}{m} [\sum_{i=1}^{m} (y_i-b) \vec{x}_i - \sum_{i=1}^{m} \vec{w} \cdot \vec{x}_i^2 ] \\

= -\frac{2}{m} [\sum_{i=1}^{m} (y_i-b) \vec{x}_i - \vec{w} \sum_{i=1}^{m} \vec{x}_i^2 ] \\

= \frac{2}{m} [\vec{w} \sum_{i=1}^{m} \vec{x}_i^2 - \sum_{i=1}^{m} (y_i-b) \vec{x}_i] \\
$$

计算MSE关于b的偏导

$$
\tag{7}
\frac{\partial MSE}{ \partial b} =  \frac{ \partial[\frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]  }{ \partial b} \\
= \frac{1}{m} \sum_{i=1}^{m} 2(y_i - \vec{w} \cdot \vec{x}_i - b)(-1)  \\
= -\frac{2}{m} [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) - \sum_{i=1}^{m}b]  \\
= -\frac{2}{m} [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) - mb]  \\
= \frac{2}{m} [mb - \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i)] \\
$$

将(6)带入(4)得
$$
\frac{\partial MSE}{ \partial w} = 0 \\
\vec{w} \sum_{i=1}^{m} \vec{x}_i^2 - \sum_{i=1}^{m} (y_i-b) \vec{x}_i = 0 \\
\vec{w} \sum_{i=1}^{m} \vec{x}_i^2 = \sum_{i=1}^{m} (y_i-b) \vec{x}_i \\
\vec{w}  = \frac{\sum_{i=1}^{m} (y_i-b) \vec{x}_i}{\sum_{i=1}^{m} \vec{x}_i^2} \\
$$
将(7)带入(5)得
$$
\frac{\partial MSE}{ \partial b} = 0 \\
mb - \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) = 0 \\
b = \frac{1}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) \\
$$




$$
\vec{w}  = \frac{\sum_{i=1}^{m} (y_i-b) \vec{x}_i}{\sum_{i=1}^{m} \vec{x}_i^2} \\
 
$$

$$
\begin{aligned}

\vec{w} \sum \vec{x}_i^2 

& = \sum (y_i \vec{x}_i - b \vec{x}_i)  \\
 
& = \sum y_i \vec{x}_i - \sum \vec{x}_i b  \\
 
& = \sum y_i \vec{x}_i - \sum \frac{\vec{x}_i}{m}  \sum (y_i - \vec{w} \cdot \vec{x}_i)\\
 
& = \sum y_i \vec{x}_i - \sum \frac{\vec{x}_i}{m} [\sum y_i - \sum \vec{w} \cdot \vec{x}_i]  \\
 
& =\sum y_i \vec{x}_i - \sum \frac{\vec{x}_i}{m} \sum y_i + \sum \frac{\vec{x}_i}{m} \sum \vec{w} \cdot \vec{x}_i \\
 
& = \sum y_i \vec{x}_i - \overline{x} \sum y_i + \frac{\vec{w}}{m} (\sum \vec{x}_i)^2 \\
 
\end{aligned} \\
$$

$$
\vec{w}( \sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2 ) = \sum y_i \vec{x}_i - \overline{x} \sum y_i \\
\vec{w} = \frac{\sum y_i (\vec{x}_i - \overline{x}) }{\sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2}
$$

最终解得

$$
\begin{equation}
\left\{
\begin{aligned}

\vec{w} = \frac{\sum y_i (\vec{x}_i - \overline{x}) }{\sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2} \\

b = \frac{1}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i)

\end{aligned}
\right.
\end{equation}
$$



### 实例

小明在一个市场的多个店铺购买了某种水果很多次，知道每次购买的个数和总价，想知道水果的市场单价w和杂费b。

```python
import numpy as np
import matplotlib.pyplot as plt


class Linear:
    def __init__(self):
        self.w = 0
        self.b = 0
    
    def foward(self, x):
        return self.w * x + self.b
    
    def learning(self, x, y):
        a = np.sum(y * (x - np.mean(x)))
        b = np.sum(np.square(x)) - np.square(np.sum(x)) / len(x)

        self.w = a / b
        self.b = np.mean(y) - self.w * np.mean(x)
        return 0

# 构造数据
x = np.arange(1, 100 ,2)
noise = np.random.normal(loc=0, scale=10, size=50)   
y = 2 * x + 10 + noise
# 初始化模型
model = Linear()
# 训练模型
model.learning(x, y)
# 查看结果
print(model.w, model.b)
plt.plot(x, model.foward(x), c='red')
plt.scatter(x, y)
plt.plot(x, 2*x+10, c='black')
plt.show()
```



## 梯度下降法

梯度是一个向量，是函数关于每一个自变量的偏导组成的向量，函数在某一点的梯度代表了函数在该点处函数值增加最快的方向。

MSE的梯度：
$$
(\frac{\partial MSE}{ \partial w}, \frac{\partial MSE}{ \partial b})
$$
求函数的最小值，就可以从一个初始点开始，每次沿着当前梯度的反方向移动一定步长，多次移动后到达最小值点。

梯度下降更新参数
$$
w_{new} = w_{old} - \alpha \frac{\partial MSE}{ \partial w} \\
b_{new} = b_{old} - \alpha \frac{\partial MSE}{ \partial b}
$$

$$
\frac{\partial MSE}{ \partial w} = -\frac{2}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b) \vec{x}_i  \\  \\
\frac{\partial MSE}{ \partial b} = -\frac{2}{m} [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) - mb]
$$

使用梯度下降法解决上一节的实例

```python
import numpy as np
import matplotlib.pyplot as plt


# 定义模型
class Linear:
    def __init__(self):
        self.w = np.array([0])
        self.b = np.array([0])
    
    def forward(self, x):
        return self.w * x + self.b
    
    def plot(self, x, y):
        plt.cla()
        plt.scatter(x, y, c='c')
        plt.plot(x, self.forward(x), c='red')
        plt.show()
        return 0
    
# 定义损失函数
def mse(y_real, y_predict):
    return np.mean(np.square(y_real - y_predict))


# 实现训练过程
def training(model, x, y, epochs, a):
    
    records_of_loss = []  # 记录训练过程中每次迭代的损失
    
    for epoch in range(epochs):
        
        model.plot(x, y)
        
        # 计算当前迭代次数时模型的输出
        y_ = model.forward(x)
        
        # 计算输出和标签间的损失
        loss = mse(y, y_)
        records_of_loss.append(loss)
        
        # 使用梯度下降法更新模型的参数
        w = model.w.copy()
        b = model.b.copy()
        
        partial_mse_w = -2 / len(x) * np.sum(y-w*x-b)
        partial_mse_b = -2 / len(x) * (np.sum(y-w*x)- len(x)*b)
        
        model.w = w - a * partial_mse_w
        model.b = b - a * partial_mse_b
    
    return records_of_loss


# 构造数据
x = np.arange(1, 100 ,2)
noise = np.random.normal(loc=0, scale=10, size=50)   
y = 2 * x + 10 + noise
# 初始化模型
model = Linear()
# 训练模型
records = training(model, x, y, 10, 0.005)
# 查看训练过程中损失的变化
plt.cla()
plt.plot(range(10), records, c='c')
plt.show()
```

