# 线性回归

### 模型定义

给定m个样本，每个样本包含n个特征的输入x，标签y是连续值

线性回归模型定义为：
$$
\hat{y} = \sum_{i=1}^{n} w_ix_i + b \\
=\vec{w} \cdot \vec{x} + b \tag{1}
$$

### 误差评价

误差，去度量y和y_hat之间的误差

MSE（Mean Square Error）均方差：
$$
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 \tag{2} 
$$

### 使用最小二乘法求解参数

由于输入和输出是给定的，MSE实际上是关于参数w和b的函数，将(1)带入(2)可得
$$
MSE(w, b) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2  \tag{3}
$$
我们的目的是使均方差最小，就是求w和b使得MSE最小。

更进一步，MSE是关于w和b的凸函数，当其关于w和b的偏导数均为0时，MSE取得最小值。即求w和b使得
$$
\frac{\partial MSE}{ \partial w} = 0 \tag{4}
$$

$$
\frac{\partial MSE}{ \partial b} = 0 \tag{5}
$$

可以解得：
$$
\begin{equation}
\left\{
\begin{aligned}

\vec{w} = \frac{\sum y_i (\vec{x}_i - \overline{x}) }{\sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2} \\

b = \frac{1}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i)

\end{aligned}
\right.
\end{equation}
$$

推导过程如下：

计算MSE关于w的偏导
$$
\tag{6}

\frac{\partial MSE}{ \partial w} 
= \frac{ \partial[\frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]  }{ \partial w} \\

= \frac{1}{m} \frac{\partial [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]}{\partial w} \\

= \frac{1}{m}  \sum_{i=1}^{m} 2(y_i - \vec{w} \cdot \vec{x}_i - b)(-\vec{x}_i) \\ 

= -\frac{2}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b) \vec{x}_i  \\ 

= -\frac{2}{m} \sum_{i=1}^{m} (y_i \vec{x}_i - b \vec{x}_i - \vec{w} \cdot \vec{x}_i^2 )  \\

= -\frac{2}{m} [\sum_{i=1}^{m} (y_i-b) \vec{x}_i - \sum_{i=1}^{m} \vec{w} \cdot \vec{x}_i^2 ] \\

= -\frac{2}{m} [\sum_{i=1}^{m} (y_i-b) \vec{x}_i - \vec{w} \sum_{i=1}^{m} \vec{x}_i^2 ] \\

= \frac{2}{m} [\vec{w} \sum_{i=1}^{m} \vec{x}_i^2 - \sum_{i=1}^{m} (y_i-b) \vec{x}_i] \\
$$

计算MSE关于b的偏导

$$
\tag{7}
\frac{\partial MSE}{ \partial b} =  \frac{ \partial[\frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]  }{ \partial b} \\
= \frac{1}{m} \sum_{i=1}^{m} 2(y_i - \vec{w} \cdot \vec{x}_i - b)(-1)  \\
= -\frac{2}{m} [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) - \sum_{i=1}^{m}b]  \\
= -\frac{2}{m} [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) - mb]  \\
= \frac{2}{m} [mb - \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i)] \\
$$

将(6)带入(4)得
$$
\frac{\partial MSE}{ \partial w} = 0 \\
\vec{w} \sum_{i=1}^{m} \vec{x}_i^2 - \sum_{i=1}^{m} (y_i-b) \vec{x}_i = 0 \\
\vec{w} \sum_{i=1}^{m} \vec{x}_i^2 = \sum_{i=1}^{m} (y_i-b) \vec{x}_i \\
\vec{w}  = \frac{\sum_{i=1}^{m} (y_i-b) \vec{x}_i}{\sum_{i=1}^{m} \vec{x}_i^2} \\
$$
将(7)带入(5)得
$$
\frac{\partial MSE}{ \partial b} = 0 \\
mb - \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) = 0 \\
b = \frac{1}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i) \\
$$




$$
\vec{w}  = \frac{\sum_{i=1}^{m} (y_i-b) \vec{x}_i}{\sum_{i=1}^{m} \vec{x}_i^2} \\
 
$$

$$
\begin{aligned}

\vec{w} \sum \vec{x}_i^2 

& = \sum (y_i \vec{x}_i - b \vec{x}_i)  \\
 
& = \sum y_i \vec{x}_i - \sum \vec{x}_i b  \\
 
& = \sum y_i \vec{x}_i - \sum \frac{\vec{x}_i}{m}  \sum (y_i - \vec{w} \cdot \vec{x}_i)\\
 
& = \sum y_i \vec{x}_i - \sum \frac{\vec{x}_i}{m} [\sum y_i - \sum \vec{w} \cdot \vec{x}_i]  \\
 
& =\sum y_i \vec{x}_i - \sum \frac{\vec{x}_i}{m} \sum y_i + \sum \frac{\vec{x}_i}{m} \sum \vec{w} \cdot \vec{x}_i \\
 
& = \sum y_i \vec{x}_i - \overline{x} \sum y_i + \frac{\vec{w}}{m} (\sum \vec{x}_i)^2 \\
 
\end{aligned} \\
$$

$$
\vec{w}( \sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2 ) = \sum y_i \vec{x}_i - \overline{x} \sum y_i \\
\vec{w} = \frac{\sum y_i (\vec{x}_i - \overline{x}) }{\sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2}
$$

最终解得

$$
\begin{equation}
\left\{
\begin{aligned}

\vec{w} = \frac{\sum y_i (\vec{x}_i - \overline{x}) }{\sum \vec{x}_i^2 - \frac{1}{m} (\sum \vec{x}_i)^2} \\

b = \frac{1}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i)

\end{aligned}
\right.
\end{equation}
$$



### 实例





```python
import numpy as np
import matplotlib.pyplot as plt


class Linear:
    def __init__(self):
        self.w = 0
        self.b = 0
    
    def foward(self, x):
        return self.w * x + self.b
    
    def learning(self, x, y):
        a = np.sum(y * (x - np.mean(x)))
        b = np.sum(np.square(x)) - np.square(np.sum(x)) / len(x)

        self.w = a / b
        self.b = np.mean(y) - self.w * np.mean(x)
        return 0

# 构造数据
x = np.arange(1, 100 ,2)
noise = np.random.normal(loc=0, scale=10, size=50)   
y = 2 * x + 10 + noise
# 初始化模型
model = Linear()
# 训练模型
model.learning(x, y)
# 查看结果
print(model.w, model.b)
plt.plot(x, model.foward(x), c='red')
plt.scatter(x, y_)
plt.plot(x, 2*x+10, c='black')
plt.show()
```



