# 机器学习问题的基本概念

样本，特征，标签，模型，学习算法，损失函数

我们要去买水果，我们挑选水果，根据水果的大小，颜色，形状来挑选，这些就是水果的特征，也叫属性。我们是根据这些特征来判断水果的甜度，或者是“好”，“坏”，这些就是水果的标签，标签可以分为连续值和离散值。

我们将一条包含水果的特征和其对应的标签的数据叫做一个样本，也叫做实例，这样的多个样本集合起来构成了一个数据集。

我们希望能够建立一个函数映射关系y_hat=f(x)，x是特征，y是标签，f就是模型。

我们如何找到合适的f的过程，称为学习，或者叫做训练。

损失函数就是度量模型输出值y_hat和标签y之间的差异的函数。

通常我们会把数据集分为两个部分，一个训练集，一个测试集，训练集用来训练模型的，测试集不参与模型的训练过程，是用于在训练结束之后来评价模型的效果。

样本 sample 

特征 feature

标签 label

数据集 data set

模型 model

学习 leaning 

训练 training

损失函数 loss function

# 感知机解决线性二分类问题

给定一个具有n个特征的输入x，感知机模型可以定义为：
$$
\hat{y} = sign(\sum_{i=1}^{n} w_ix_i + b)
$$
用向量来表示x和w
$$
\vec{x} = (x_1, ... , x_n)  \\
\vec{w} = (w_1, ... , w_n)  \\
$$
感知机模型就可以写为：
$$
\hat{y} = sgin(\vec{w} \cdot \vec{x} + b)
$$
我们的标签定义为1和-1。

我们可以定义一个超平面，或者叫决策面：
$$
\vec{w} \cdot \vec{x} + b = 0
$$
为了便于理解，我们考虑特征只有2维的情况，
$$
\vec{x} = (x_1, x_2) \\
w_1 * x_1 + w_2 * x_2 + b = 0
$$
我们可以把它转变为斜率和截距的形式
$$
x_2 = - \frac{w_1}{w_2} * x_1 - \frac{b}{w_2}
$$
若空间中的一个点(x1, x2)在这个决策面的上方，显然
$$
w_1 * x_1 + w_2 * x_2 + b > 0
$$
此时，经过符号函数映射之后，感知机输出1

同理，若该点在决策面下方，经过符号函数映射之后，感知机输出-1

所以说，我们要找到x到y的合理映射，就能转变为找到一个合适的决策面，它从空间中将正负样本点分隔在决策面的不同的两侧。

我们考虑模型判断出错的情况

| 编号 | 标签y | 模型输出y_hat | y-y_hat |
| ---- | ----- | ------------- | ------- |
| a    | 1     | 1             | 0       |
| b    | -1    | -1            | 0       |
| c    | 1     | -1            | 2       |
| d    | -1    | 1             | -2      |

出错时，实际上是y和y_hat异号
$$
y * \hat{y} < 0 \\
y * (w_1 * x_1 + w_2 * x_2 + b) < 0
$$
对于情况c，我们想要纠正w和b使得
$$
(w_1 * x_1 + w_2 * x_2 + b) > 0
$$
若w和x同号，也就是说要按x的方向纠正w，即w+x

对于情况d，我们想要纠正w和b使得
$$
(w_1 * x_1 + w_2 * x_2 + b) < 0
$$
若w和x异号，也就是说要按x的反方向纠正w，即w-x

综上，我们就可以得到w的更新方法，

```python
if y == 1 and y_hat == -1:
	w = w + x
elif y == -1 and y_hat == 1:
	w = w - x
```

```python
w = w + a * (y - y_hat) * x
```

统一为
$$
w_{new} = w_{old} + \alpha * (y- \hat{y}) * x
$$







# 线性回归

模型：
$$
\hat{y} = \sum_{i=1}^{n} w_ix_i + b
$$
MAE 平均绝对值误差

误差，去度量y和y_hat之间的误差

MSE（Mean Square Error）均方差：
$$
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 \\
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \sum_{i=1}^{n} w_ix_i - b)^2 \\
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2
$$
我们的目的是使均方差最小，就是求w和b使得MSE最小，即求w和b使得

part 0
$$
\frac{\partial MSE}{ \partial w} = 0 \\
\frac{\partial MSE}{ \partial b} = 0
$$

part 1
$$
\frac{\partial MSE}{ \partial w} = \frac{ \partial[\frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]  }{ \partial w} \\

= \frac{1}{m} \frac{\partial [\sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]}{\partial w} \\

= -\frac{2}{m}  \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b) \vec{x}_i  \\ 

= -\frac{2}{m} \sum_{i=1}^{m} (y_i \vec{x}_i - \vec{w} \cdot \vec{x}_i^2 - b \vec{x}_i)  \\

= -\frac{2}{m} ( \sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m}\vec{w} \cdot \vec{x}_i^2 - \sum_{i=1}^{m}b \vec{x}_i)  \\
$$

part 2

$$
\frac{\partial MSE}{ \partial b} =  \frac{ \partial[\frac{1}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)^2]  }{ \partial b} \\
= -\frac{2}{m} \sum_{i=1}^{m} (y_i - \vec{w} \cdot \vec{x}_i - b)  \\
= -\frac{2}{m} (\sum_{i=1}^{m} y_i - \sum_{i=1}^{m} \vec{w} \cdot \vec{x}_i - \sum_{i=1}^{m} b) \\
= -\frac{2}{m} (\sum_{i=1}^{m} y_i - \sum_{i=1}^{m} \vec{w} \cdot \vec{x}_i - m b)
$$

part 3

将part 2带入part 1得
$$
\frac{\partial MSE}{ \partial b} = 0 \\
(\sum_{i=1}^{m} y_i - \sum_{i=1}^{m} \vec{w} \cdot \vec{x}_i - m b) = 0 \\
mb = \sum_{i=1}^{m} y_i - \sum_{i=1}^{m} \vec{w} \cdot \vec{x}_i
\\
b  = \underline{y} - \vec{w} \cdot \vec{\underline{x}_i}
$$

将part3 带入part1得


$$
( \sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m}\vec{w} \cdot \vec{x}_i^2 - \sum_{i=1}^{m}b \vec{x}_i) = 0 \\

( \sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m}\vec{w} \cdot \vec{x}_i^2 - \sum_{i=1}^{m} (\underline{y} - \vec{w} \cdot \vec{\underline{x}_i}) \vec{x}_i) = 0 \\

( \sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m}\vec{w} \cdot \vec{x}_i^2 - \sum_{i=1}^{m} (\underline{y}\vec{x}_i - \vec{w} \cdot \vec{\underline{x}_i} \vec{x}_i)) = 0 \\

( \sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m}\vec{w} \cdot \vec{x}_i^2 - \sum_{i=1}^{m} \underline{y}\vec{x}_i - \sum_{i=1}^{m} \vec{w} \vec{\underline{x}_i} \vec{x}_i) = 0 \\

\sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m} \underline{y}\vec{x}_i  = \sum_{i=1}^{m}\vec{w} \cdot \vec{x}_i^2 + \sum_{i=1}^{m} \vec{w}  \vec{\underline{x}_i} \vec{x}_i \\

\sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m} \underline{y}\vec{x}_i  = \vec{w} (\sum_{i=1}^{m} \vec{x}_i^2 + \sum_{i=1}^{m} \vec{\underline{x}_i} \vec{x}_i) \\

\vec{w} = \frac{\sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m} \underline{y}\vec{x}_i}{\sum_{i=1}^{m}  \vec{x}_i^2 + \sum_{i=1}^{m} \vec{\underline{x}_i} \vec{x}_i} \\

\vec{w} = \frac{\sum_{i=1}^{m} y_i \vec{x}_i - \sum_{i=1}^{m}y_i \vec{\underline{x}_i}}
{\sum_{i=1}^{m} \vec{x}_i^2 + \frac{1}{m}(\sum_{i=1}^{m}\vec{x}_i)^2 } \\

\vec{w} = \frac{\sum_{i=1}^{m} y_i (\vec{x}_i - \vec{\underline{x}_i})}
{\sum_{i=1}^{m} \vec{x}_i^2 + \frac{1}{m}(\sum_{i=1}^{m}\vec{x}_i)^2 }
$$






$$
\vec{w} = \frac{\sum_{i=1}^{m} y_i (\vec{x}_i - \vec{\underline{x}_i})}
{\sum_{i=1}^{m} \vec{x}_i^2 + \frac{1}{m}(\sum_{i=1}^{m}\vec{x}_i)^2 } \\
b  = \underline{y} - \vec{w} \cdot \vec{\underline{x}_i}
$$






```python
import numpy as np
import matplotlib.pyplot as plt


class Linear:
    def __init__(self):
        self.w = 0
        self.b = 0
    
    def foward(self, x):
        return self.w * x + self.b
    
    def learning(self, x, y):
        a = np.sum(y * (x - np.mean(x)))
        b = np.sum(np.square(x)) - np.square(np.sum(x)) / len(x)

        self.w = a / b
        self.b = np.mean(y) - self.w * np.mean(x)
        return 0

# 构造数据
x = np.arange(1, 100 ,2)
noise = np.random.normal(loc=0, scale=10, size=50)   
y = 2 * x + 10 + noise
# 初始化模型
model = Linear()
# 训练模型
model.learning(x, y)
# 查看结果
print(model.w, model.b)
plt.plot(x, model.foward(x), c='red')
plt.scatter(x, y_)
plt.plot(x, 2*x+10, c='black')
plt.show()
```















