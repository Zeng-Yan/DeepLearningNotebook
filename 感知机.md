# 感知机解决线性二分类问题

给定一个具有n个特征的输入x，感知机模型可以定义为：
$$
\hat{y} = sign(\sum_{i=1}^{n} w_ix_i + b)
$$
用向量来表示x和w
$$
\vec{x} = (x_1, ... , x_n)  \\
\vec{w} = (w_1, ... , w_n)  \\
$$
感知机模型就可以写为：
$$
\hat{y} = sgin(\vec{w} \cdot \vec{x} + b)
$$
我们的标签定义为1和-1。

我们可以定义一个超平面，或者叫决策面：
$$
\vec{w} \cdot \vec{x} + b = 0
$$
为了便于理解，我们考虑特征只有2维的情况，
$$
w_1 * x_1 + w_2 * x_2 + b = 0
$$
我们可以把它转变为斜率和截距的形式
$$
x_2 = - \frac{w_1}{w_2} * x_1 - \frac{b}{w_2}
$$
若空间中的一个点(x1, x2)在这个决策面的上方，显然
$$
w_1 * x_1 + w_2 * x_2 + b > 0
$$
此时，经过符号函数映射之后，感知机输出1

同理，若该点在决策面下方，经过符号函数映射之后，感知机输出-1

所以说，我们要找到x到y的合理映射，就能转变为找到一个合适的决策面，它从空间中将正负样本点分隔在决策面的不同的两侧。

我们考虑模型判断出错的情况

| 编号 | 标签y | 模型输出y_hat | y-y_hat |
| ---- | ----- | ------------- | ------- |
| a    | 1     | 1             | 0       |
| b    | -1    | -1            | 0       |
| c    | 1     | -1            | 2       |
| d    | -1    | 1             | -2      |

出错时，实际上是y和y_hat异号
$$
y * \hat{y} < 0 \\
y * (w_1 * x_1 + w_2 * x_2 + b) < 0
$$
对于情况c，我们想要纠正w和b使得
$$
(w_1 * x_1 + w_2 * x_2 + b) > 0
$$
若w和x同号，也就是说要按x的方向纠正w，即w+x

对于情况d，我们想要纠正w和b使得
$$
(w_1 * x_1 + w_2 * x_2 + b) < 0
$$
若w和x异号，也就是说要按x的反方向纠正w，即w-x

统一为
$$
w_{new} = w_{old} + \alpha * (y- \hat{y}) * x
$$
